import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
from streamlit_ace import st_ace
from code_editor import code_editor
import math

from scipy.integrate import odeint
from scipy.optimize import fsolve
import time

import time
import io
import sys


def general_rules():
    st.header("1. Общие правила вычислительной работы")

    st.markdown("""
        Практика вычислительной работы показывает, что при её выполнении оказываются полезными некоторые общие правила, 
        следование которым помогает избежать характерных ошибок.

        **Правило 1**. Прежде чем решать задачу, полезно задать вопрос: для чего нам нужен ответ, что мы с ним желаем сделать? 
        Этот вопрос при тщательном обдумывании ответа на него позволяет правильно спланировать объём и содержание выходной информации, 
        не упуская важные детали вычислений, позволяющие оценить достоверность и точность получаемых результатов.

        **Правило 2**. Очень важно в ходе вычислений использовать известную входную информацию о решении. 
        Часто дополнительная информация позволяет упростить решение задачи и контролировать правильность и точность вычислений.

        **Правило 3**. Перед разработкой программ (или проведением расчётов) полезно поставить аналитически или качественно предлагаему задачу.
    """)

    st.markdown("""
        **Правило 4**. Разработку вычислительной схемы полезно разделить на 3 этапа:
        1. **Выбор вычислительного метода**.
        2. **Промежуточный контроль результатов**.
        3. **Оценка точности полученного результата**.
    """)

    st.markdown("""
        **4.1. Выбор вычислительного метода**. Этот этап опирается на информацию, полученную в рамках предыдущих пунктов, 
        и играет определяющую роль для успешного решения задачи. Важно учитывать, что с увеличением мощности вычислительных машин 
        нередко к выбору метода начинают относиться менее критично, полагаясь на быстродействие машин. Однако неправильный выбор метода 
        может привести к значительным задержкам или даже невозможности завершения вычислений в разумные сроки. Например, выбор метода 
        Гаусса для больших матриц может существенно сократить время расчёта по сравнению с методом полного перебора.
    """)

    st.markdown("""
        Пример: пусть требуется вычислить определитель (детерминант) матрицы порядка \(N\). 
        Для этого используется метод полного перебора, когда каждый элемент матрицы умножается и суммируется последовательно.
        Число операций (сложений и умножений) будет равно:
    """)

    st.latex(r'''
    M = N!(сложений) + N \cdot N!(умножений) = (N + 1)!
    ''')

    st.markdown("""
        Для больших N>10 можно использовать приближённую формулу Стирлинга:
    """)

    st.latex(r'''
    M = \left( N+1 \right)^{N+1} \cdot e^{-N-1} \cdot \sqrt{2 \pi (N+1)}
    ''')

    st.markdown("""
        Рассмотрим теперь вычисления для \(N=99\). Число операций может составить:
    """)

    st.latex(r'''
    M = 10^{158}
    ''')

    st.markdown("""
        Предположим, что машина выполняет элементарную операцию за время:
    """)

    st.latex(r'''
    3 \cdot 10^{-18} \, \text{секунд.}
    ''')

    st.markdown("""
        Тогда время, необходимое для выполнения всех операций, составит:
    """)

    st.latex(r'''
    \tau_э = \frac{M}{m} = 10^{92} \, лет
    ''')

    st.markdown("""
        Таким образом, даже при использовании самых быстрых вычислительных машин, задача не будет выполнена в обозримое время. 
        Поэтому выбор более эффективного метода, например, метода Гаусса, позволяет сократить время вычислений до нескольких долей секунд.
    """)

    st.markdown("""
        **4.2. Промежуточный контроль результатов**. В ходе вычислений необходимо предусматривать промежуточные этапы контроля. 
        Этот контроль помогает убедиться, что вычисления идут в правильном направлении, и выявить возможные ошибки на ранних стадиях. 
        Опыт показывает, что игнорирование промежуточного контроля может привести к значительным искажениям конечных результатов.
    """)

    st.markdown("""
        **4.3. Оценка точности конечного результата**. После получения окончательного результата важно провести его оценку. 
        Необходимо понять, насколько точно решение задачи отвечает исходным условиям и можно ли доверять результатам, полученным в ходе вычислений. 
        На этом этапе полезно знать характеристики используемых методов, чтобы оценить возможные численные погрешности и влияние внешних факторов на точность.
    """)


def inaccuracy():
    st.header("2. Источники и классификация погрешности")

    st.markdown("""
    В процессе численных расчетов погрешности могут возникать из различных источников и иметь различную природу. Классификация погрешностей включает в себя:

    - **Абсолютная погрешность:** Разница между точным значением и приближенным, измеренным или вычисленным значением.
    - **Относительная погрешность:** Отношение абсолютной погрешности к точному значению, часто выражается в процентах.
    - **Погрешность действий:** Погрешности, возникающие в результате выполнения арифметических операций, особенно при работе с числами с плавающей запятой в компьютерных вычислениях.

    ### Общая формула погрешности
    Общая формула для абсолютной и относительной погрешности может быть выражена в LaTeX как:
    """)

    st.latex(r"""
    \text{Абсолютная погрешность:} \quad \Delta x = |x_{\text{точное}} - x_{\text{приближенное}}|
    """)

    st.latex(r"""
    \text{Относительная погрешность:} \quad \varepsilon = \frac{\Delta x}{|x_{\text{точное}}|}
    """)

    st.latex(r"""
    \text{Где} \ (\Delta x) \ \text{— абсолютная погрешность,} \ (\varepsilon) \ \text{— относительная погрешность,}\\ \ (x_{\text{точное}}) \ \text{— точное значение,} \ (x_{\text{приближенное}}) \ \text{— приближенное значение.}
    """)

    st.markdown("""
    Понимание и учет этих погрешностей критически важно для достижения точности и надежности численных методов.
    """)


def general_rules_for_approximating_functions():
    st.subheader("3.1. Общие правила")

    st.markdown("""
        При решении задач численного анализа часто возникает необходимость выполнения той или иной операции (дифференцирования, интегрирования, поиска экстремума и т.п.) над функцией, значения которой известны лишь при нескольких значениях аргумента (называемых узлами функции). Общий подход к решению таких задач заключается в восстановлении функции по имеющимся узлам и значениям. По значениям в узлах происходит восстановление функции во всей интересующей области, исходя из некоторых предположений о виде функции. 
        Затем над полученной (интерполирующей или аппроксимирующей) функцией производятся нужные действия. 
        При этом возникают четыре вопроса:

        1. Какие узлы следует использовать?
        2. Какой класс интерполирующих (аппроксимирующих) функций следует выбирать?
        3. Как применять критерий согласия?
        4. Какую точность следует ожидать?
    """)

    st.markdown("""
        Рассмотрим кратко каждый из этих вопросов. Чтобы увидеть, что проблема выбора узлов важна, предположим, что требуется вычислить интеграл:
    """)

    st.latex(r'''
    \int_0^1 f(x) \, dx,
    ''')

    st.markdown("""
        имея в распоряжении лишь несколько узлов функции. Если бы мы знали детальное поведение функции f(x) , 
        то могли бы расположить узлы наилучшим образом под поведение  f(x) . Однако в случае, если поведение функции неизвестно, различные варианты выбора узлов дадут разные результаты.

        В численном анализе широко применяются три класса функций для аппроксимации:

        - Линейные комбинации функций""")
    st.latex(r'''
        1, x, x^2, \dots, x^n 
        ''')
    st.markdown("""
            (класс многочленов степени n);
        - Линейные комбинации тригонометрических функций 
        """)
    st.latex(r'''
        \cos a_i x,\quad \sin a_i x,\quad  i = 1, 2, \dots, n ; 
        ''')
    st.markdown("""
        - Линейные комбинации показательных функций """)
    st.latex(r'''
        e^{-a_i x}, \quad i = 1, 2, \dots, n. 
        ''')

    st.markdown("""
        Каждый из этих классов функций обладает важным свойством инвариантности относительно сдвига аргумента. Если функция \( P(x) \) принадлежит одному из этих классов, то и функция \( P(x + x_0) \) также принадлежит этому классу. Это означает, что выбор аппроксимирующих функций не требует введения особых сведений о начальном отсчёте \( x \).

        Классическим ответом на вопрос о критерии согласия является точное совпадение аппроксимирующей функции с исходной в узловых точках. Другой распространённый критерий — минимизация квадратичного отклонения аппроксимирующей функции от исходных значений. Третий критерий — минимизация максимального отклонения (критерий Чебышёва).

        Для иллюстрации точности рассмотрим систему линейных алгебраических уравнений""")
    st.latex(r'''
     A \overline {x} = \overline{b} 
     ''')

    st.markdown(""" где можно применить следующие критерии точности:""")

    st.markdown(""" 
    - Значения $${x_i}$$ должны точно удовлетворять системе;
    - Остатки уравнений при подстановке значений $${x}$$ должны быть малы;
    - Данные уравнения должны быть близки к исходным уравнениям, для которых $${x_i}\ - $$ точные решения.
    """)

    st.latex(r'''
        ''')
    st.latex(r'''
    ''')


# 3.2.1. Метод Лагранжа
def lagrange_for_interpolation():
    st.header("3.2.1. Метод Лагранжа для интерполяции")

    st.markdown("""
        Метод Лагранжа — это форма полиномиальной интерполяции, используемая для аппроксимации функций. Полином Лагранжа
         представляет собой линейную комбинацию базисных полиномов Лагранжа, что позволяет точно проходить через 
         заданные точки.
        """)

    st.latex(r"""
        \textbf{Теоретическая основа:} \\
        Полином \, Лагранжа \, L(x) \, для \, n \, точек \, задаётся \, формулой: \\
        L(x) = \sum_{i=0}^{n-1} y_i \ell_i(x)
        """)
    st.latex(r"""
        где \, \ell_i(x) \, — базисные \, полиномы \, Лагранжа, \, определённые \, как: \\
        \ell_i(x) = \prod_{\substack{j=0 \\ j \neq i}}^{n-1} \frac{x - x_j}{x_i - x_j}
        """)

    st.markdown("**Реализация на чистом Python:**")
    st.code("""
        def lagrange_interpolation(x, x_points, y_points):
        '''Метод Лагранжа для интерполяци'''
        total = 0
        n = len(x_points)
        for i in range(n):
            term = y_points[i]
            for j in range(n):
                if i != j:
                    term *= (x - x_points[j]) / (x_points[i] - x_points[j])
            total += term
        return total
        """)

    def lagrange_interpolation(x, x_points, y_points):
        """Метод Лагранжа для интерполяции"""
        total = 0
        n = len(x_points)
        for i in range(n):
            term = y_points[i]
            for j in range(n):
                if i != j:
                    term *= (x - x_points[j]) / (x_points[i] - x_points[j])
            total += term
        return total

    # Функция для демонстрации полиномов Лагранжа
    def plot_lagrange_interpolations():
        # Определяем узлы интерполяции и значения функции (например, f(x) = 1 / (1 + x^2))
        x_points_2 = np.linspace(-1, 1, 3)  # Узлы для полинома степени 2
        y_points_2 = 1 / (1 + x_points_2 ** 2)  # Значения функции для степени 2

        x_points_4 = np.linspace(-1, 1, 5)  # Узлы для полинома степени 4
        y_points_4 = 1 / (1 + x_points_4 ** 2)  # Значения функции для степени 4

        x_points_8 = np.linspace(-1, 1, 9)  # Узлы для полинома степени 8
        y_points_8 = 1 / (1 + x_points_8 ** 2)  # Значения функции для степени 8

        # Создаём значения для x на интервале [-1.5, 1.5]
        x_values = np.linspace(-1.5, 1.5, 500)
        y_values = 1 / (1 + x_values ** 2)  # Истинная функция f(x) = 1 / (1 + x^2)

        # Строим график
        plt.figure(figsize=(10, 6))
        plt.plot(x_values, y_values, label="f(x) = 1 / (1 + x^2)", color="blue", linewidth=2)  # Истинная функция

        # Строим полиномы Лагранжа для каждой степени
        y_interp_2 = [lagrange_interpolation(x, x_points_2, y_points_2) for x in x_values]
        y_interp_4 = [lagrange_interpolation(x, x_points_4, y_points_4) for x in x_values]
        y_interp_8 = [lagrange_interpolation(x, x_points_8, y_points_8) for x in x_values]

        plt.plot(x_values, y_interp_2, label="P2(x) (Lagrange degree 2)", linestyle="--", color="orange", linewidth=2)
        plt.plot(x_values, y_interp_4, label="P4(x) (Lagrange degree 4)", linestyle="--", color="green", linewidth=2)
        plt.plot(x_values, y_interp_8, label="P8(x) (Lagrange degree 8)", linestyle="--", color="red", linewidth=2)

        # Добавляем узлы интерполяции на график
        plt.scatter(x_points_2, y_points_2, color='orange', zorder=5, label="Nodes for P2(x)")
        plt.scatter(x_points_4, y_points_4, color='green', zorder=5, label="Nodes for P4(x)")
        plt.scatter(x_points_8, y_points_8, color='red', zorder=5, label="Nodes for P8(x)")

        # Настройка графика
        plt.title("Интерполяция с помощью полиномов Лагранжа")
        plt.xlabel("x")
        plt.ylabel("f(x)")
        plt.legend()
        plt.grid(True)
        st.pyplot(plt)

    # Streamlit UI
    st.header("Метод Лагранжа для интерполяции")

    st.markdown("""
    Метод Лагранжа — это форма полиномиальной интерполяции, используемая для аппроксимации функций.
    Мы будем использовать более сложную функцию $$f(x) = \\frac{1}{1 + x^2}$$ для интерполяции.

    Здесь вы можете увидеть, как полиномы Лагранжа для степеней 2, 4 и 8 приближают эту функцию.
    """)

    # Визуализация интерполяции для выбранных полиномов
    plot_lagrange_interpolations()


# 3.2.2. Метод Ньютона
def newton_for_interpolation():
    st.header("3.2.2. Метод Ньютона для интерполяции")

    st.markdown("""
        Метод Ньютона — это форма полиномиальной интерполяции, которая использует разделённые разности для построения интерполяционного полинома. Он позволяет последовательно добавлять новые узлы интерполяции без пересчёта всех коэффициентов, что делает метод более удобным для вычислений.
        """)

    st.latex(r"""
        \textbf{Теоретическая основа:} \\
        Полином \, Ньютона \, P_n(x) \, для \, n \, точек \, задаётся \, формулой: \\
        P_n(x) = f(x_0) + f[x_0, x_1](x - x_0) + \dots + f[x_0, x_1, ..., x_n](x - x_0) \cdot (x - x_1) \cdot ... \cdot (x - x_{n-1})
        """)
    st.latex(r"""
        где \, f[x_0, x_1, ..., x_k] \, — разделённые \, разности, \, определённые \, как: \\
        f[x_i] = y_i, \, f[x_i, x_{i+1}] = \frac{y_{i+1} - y_i}{x_{i+1} - x_i}, \dots
        """)

    st.markdown("**Реализация на чистом Python:**")
    st.code("""
    def divided_differences(x, y):
        #Функция для вычисления таблицы разделённых разностей.
        n = len(x)
        coef = [[0 for _ in range(n)] for _ in range(n)]
        for i in range(n):
            coef[i][0] = y[i]

        for j in range(1, n):
            for i in range(n - j):
                coef[i][j] = (coef[i + 1][j - 1] - coef[i][j - 1]) / (x[i + j] - x[i])

        return [coef[0][i] for i in range(n)]

    def newton_polynomial(x_data, y_data, x):
        #Функция для вычисления значения полинома Ньютона в точке.
        coef = divided_differences(x_data, y_data)
        n = len(coef)
        result = coef[-1]
        for i in range(n - 2, -1, -1):
            result = coef[i] + (x - x_data[i]) * result
        return result

    # Пример узлов и значений
    x_data = [1, 2, 3, 4]
    y_data = [1, 8, 27, 64]  # Значения y = x^3

    # Тестирование интерполяции в точке x = 2.5
    interpolated_value = newton_polynomial(x_data, y_data, 2.5)
    print("Интерполированное значение в x = 2.5:", interpolated_value)
        """)

    st.markdown("""
        Этот код демонстрирует реализацию метода Ньютона для интерполяции на языке Python. 
        Он использует разделённые разности для вычисления коэффициентов интерполяционного полинома и вычисления значения полинома в нужной точке.
        """)

    st.markdown("**Преимущества:**")
    st.markdown("""
        - Удобство добавления новых узлов интерполяции без пересчёта всех коэффициентов.
        - Меньшая вычислительная сложность по сравнению с другими методами при добавлении узлов.
        """)

    st.markdown("**Недостатки:**")
    st.markdown("""
        - Неустойчив при большом числе узлов (как и метод Лагранжа).
        - Зависимость от порядка введения узлов.
        """)

    st.markdown("**Алгоритм:**")
    st.latex(r"""
        1. \, Выбрать \, узлы \, интерполяции \, x_i \, и \, соответствующие \, значения \, y_i. \\
        2. \, Построить \, таблицу \, разделённых \, разностей \, f[x_i, ..., x_j]. \\
        3. \, Для \, каждого \, x \, в \, области \, определения \, вычислить \, P_n(x).
        """)

    # Демонстрация работы метода Ньютона для интерполяции с возможностью добавления новых точек

    # Функция для вычисления таблицы разделённых разностей
    def compute_divided_differences(x_data, y_data):
        n = len(x_data)
        dd_table = [[0 for _ in range(n)] for _ in range(n)]
        for i in range(n):
            dd_table[i][0] = y_data[i]
        for j in range(1, n):
            for i in range(n - j):
                dd_table[i][j] = (dd_table[i + 1][j - 1] - dd_table[i][j - 1]) / (x_data[i + j] - x_data[i])
        return dd_table

    # Функция для вычисления значения полинома Ньютона
    def newton_polynomial(x_data, y_data, x, dd_table):
        result = dd_table[0][0]
        for i in range(1, len(x_data)):
            product = 1
            for j in range(i):
                product *= (x - x_data[j])
            result += dd_table[0][i] * product
        return result

    # Заданные точки для интерполяции
    x_points = [1, 2, 3, 4, 5]
    y_points = [1, 8, 27, 64, 125]  # Значения y = x^3

    # Функция для обновления графика и таблицы
    def update_plot_and_table(x_points, y_points):
        dd_table = compute_divided_differences(x_points, y_points)
        x_range = [min(x_points) + i * (max(x_points) - min(x_points)) / 100 for i in range(101)]
        y_approx = [newton_polynomial(x_points, y_points, x, dd_table) for x in x_range]
        st.write("Таблица разделённых разностей:")
        st.dataframe(dd_table)

        fig, ax = plt.subplots()
        ax.plot(x_range, y_approx, label='Приближение Ньютона')
        ax.scatter(x_points, y_points, color='red', label='Узлы интерполяции')
        ax.legend()
        st.pyplot(fig)

    # Интерфейс для добавления новых точек
    st.subheader("Добавление новых точек для интерполяции")
    new_x = st.number_input("Введите значение x:", format="%f", step=1.0, key="x_input")
    new_y = st.number_input("Введите значение y:", format="%f", step=1.0, key="y_input")
    add_point_button = st.button("Добавить точку")

    # Обработка добавления новой точки
    if add_point_button:
        if new_x in x_points:
            st.error("Точка с таким x уже существует. Введите уникальное значение x.")
        else:
            x_points.append(new_x)
            y_points.append(new_y)

    update_plot_and_table(x_points, y_points)


# 3.2.3. Погрешность многочленной аппроксимации
def error_of_polynomial_interpolation():
    st.header("3.2.3. Погрешность многочленной аппроксимации")

    st.markdown("""
    При заданной функции y(x) в n+1 точке, можно провести через эти точки многочлен P(x), который в узлах x совпадает с y(x) с машинной точностью. Важно понимать, как сильно отличается y(x) от P(x) в точках x не совпадающих с узлами.
    """)

    # Отдельное использование st.latex для формул
    st.latex(r"""
    y(x) - P_n(x) = (x - x_1)(x - x_2) \cdots (x - x_{n+1}) \cdot K(x)
    """)

    st.markdown("""
    где K(x) — некоторая соответствующим образом определённая функция.
    """)

    st.latex(r"""
    \Phi(x) = y(x) - P_n(x) - (x - x_1) \cdots (x - x_{n+1}) \cdot K(x^*)
    """)

    st.markdown("""
    где x* — некоторое произвольное, но фиксированное значение, не совпадающее с узлами x.
    """)

    st.latex(r"""
    \Phi^{(n+1)}(x) = y^{(n+1)}(x) - (n+1)! \cdot K(x^*)
    """)

    st.latex(r"""
    \Phi^{(n+1)}(x) = 0, \quad \text{следовательно, существует такое} \, \tilde{x}, \, \text{что}
    """)

    st.latex(r"""
    y^{(n+1)}(\tilde{x}) = (n+1)! \cdot K(x^*)
    """)

    st.latex(r"""
    y(x) = P_n(x) + (x - x_1) \cdots (x - x_{n+1}) \cdot \frac{y^{(n+1)}(\tilde{x})}{(n+1)!}
    """)

    st.markdown("""
    **Пример:** Рассмотрим функцию y = log(x) по значениям в точках x = 1, 2, 3, 4. Ошибка аппроксимации будет зависеть от выбора точки x.
    """)

    # Функция логарифма и её производные
    def log_function(x):
        return np.log(x)

    # Функция для вычисления разделённых разностей
    def divided_differences(x, y):
        n = len(x)
        coef = np.zeros((n, n))
        coef[:, 0] = y
        for j in range(1, n):
            for i in range(n - j):
                coef[i, j] = (coef[i + 1, j - 1] - coef[i, j - 1]) / (x[i + j] - x[i])
        return coef[0, :]

    # Функция для вычисления значения полинома Ньютона
    def newton_polynomial(x_data, y_data, x):
        coef = divided_differences(x_data, y_data)
        n = len(coef)
        result = coef[0]
        for i in range(1, n):
            product = 1
            for j in range(i):
                product *= (x - x_data[j])
            result += coef[i] * product
        return result

    # Заданные точки для интерполяции
    x_data = np.array([1, 2, 3, 4])
    y_data = log_function(x_data)

    # Генерация точек для построения графика
    x_vals = np.linspace(1, 4, 100)
    y_vals = log_function(x_vals)
    poly_vals = [newton_polynomial(x_data, y_data, x) for x in x_vals]

    # Построение графика
    fig, ax = plt.subplots()
    ax.plot(x_vals, y_vals, label='Исходная функция log(x)')
    ax.plot(x_vals, poly_vals, label='Многочлен Ньютона')
    ax.scatter(x_data, y_data, color='red', label='Узлы интерполяции')
    ax.legend()
    st.pyplot(fig)

    st.markdown("""
    **Анализ погрешности:** В зависимости от выбора точки x, погрешность аппроксимации может значительно варьироваться. Например, в точке x = 2.5, погрешность может быть минимальной или максимальной, в зависимости от распределения узлов интерполяции и характера функции.
    """)

    # Вывод дополнительных сведений о погрешности
    def calculate_error(x, actual, predicted):
        return np.abs(actual - predicted)

    error_example_x = 2.5
    actual_log = np.log(error_example_x)
    predicted_log = newton_polynomial(x_data, y_data, error_example_x)
    error_at_x = calculate_error(error_example_x, actual_log, predicted_log)

    st.write(f"Погрешность аппроксимации в точке x = {error_example_x}: {error_at_x:.6f}")


def polynomial_approximation_difficulties():
    st.header("3.2.4. Трудности приближения многочленом")

    st.markdown(
        """    Согласно выражению для ошибки приближения многочленом (формула 9), ошибку можно выразить через производную аппроксимируемой функции.    Обычно предполагается, что для больших n эта ошибка мала, но на практике это не всегда так. Рассмотрим функцию:    """)

    # Формула для логарифмической функции
    st.latex(r"y = \ln(x)")

    st.markdown("""    Для этой функции можно записать следующую производную:    """)

    # Формула для производной функции
    st.latex(r"y^{(n+1)}(x) = \frac{(-1)^n n!}{x^n}")

    st.markdown(
        """    С увеличением n для фиксированного x производные начинают значительно возрастать. Это распространённый случай для многих функций,     у которых производные высокого порядка имеют тенденцию резко увеличиваться при росте n .     Это приводит к тому, что на некоторых интервалах аппроксимируемая функция перестает сходиться к настоящему значению.    Рассмотрим классический пример:    """)

    # Формула для функции Рунге
    st.latex(r"y(x) = \frac{1}{1 + 25x^2}")

    st.markdown(
        """    Этот пример интерполяции, предложенный Рунге, показывает, что на отрезке [-1, 1], если узлы равномерно распределены,     ошибка аппроксимации будет расти с увеличением порядка интерполяции n, особенно на концах отрезка.     Данный эффект приводит к значительным отклонениям в аппроксимируемой функции, несмотря на точное совпадение с узловыми значениями.    """)

    st.markdown(
        """    Важно отметить, что интерполяция многочленами на равномерно распределённых узлах может стать причиной роста ошибок,     особенно для функций с высокими производными на краях интервала. Это известно как **феномен Рунге**.    """)


# 3.2.5 Многочлены Чебышева
def chebyshev_polynomials():
    st.header("3.2.5. Многочлены Чебышева")

    st.markdown("""
    Многочлены Чебышёва играют важную роль в численном анализе, поэтому приведем некоторые сведения о них.

    Многочлены Чебышёва $$T_n(x)$$ определяются при $$|x| \leq 1$$ с помощью соотношения:
    """)
    st.latex(r"""
    T_n(x) = \cos(n \arccos x).
    """)
    st.markdown("""
    Можно показать, что $$T_n(x)$$ являются коэффициентами разложения в ряд по $$s$$ функции:
    """)
    st.latex(r"""
    F(s) = \frac{1 - sx}{1 - 2xs + s^2}
    """)
    st.markdown("""
    (она называется производящей функцией для этих многочленов):
    """)
    st.latex(r"""
    F(s) = \sum_{n=0}^\infty T_n(x)s^n \quad (|s| \leq 1).
    """)

    st.markdown("""
    Многочлены $$T_n(x)$$ ортогональны на отрезке $$[-1, 1]$$:
    """)
    st.latex(r"""
    \int_{-1}^1 \frac{dx}{\sqrt{1 - x^2}} {T_m(x)T_n(x)} = 
    \begin{cases} 
    0, & m \neq n, \\ 
    \frac{\pi}{2}, & m = n \neq 0, \\ 
    \pi, & m = n = 0.
    \end{cases}
    """)
    st.markdown("""
    и удовлетворяют рекуррентным соотношениям:
    """)
    st.latex(r"""
    T_{n+1}(x) + T_{n-1}(x) = 2xT_n(x), \quad n \geq 1;
    """)
    st.latex(r"""
    T_{m+n} + T_{m-n}(x) = 2T_m(x)T_n(x);
    """)
    st.latex(r"""
    \frac{dT_n}{dx} = 2nT_{n-1}(x) + \frac{n}{n-2} \frac{d}{dx}T_{n-2} \quad (n \geq 3).
    """)

    st.markdown("""
    Приведем несколько многочленов $$T_n(x)$$:
    """)
    st.latex(r"""
    T_0(x) = 1; \\
    T_1(x) = x;\\
    T_2(x) = 2x^2 - 1;\\
    T_3(x) = 4x^3 - 3x.
    """)

    st.markdown("""
    Отметим следующее свойство многочлена $$T_n(x)$$: при $$x^n$$ в нём стоит коэффициент $$2^{n-1}$$.

    Широкое приложение многочлены $$T_n(x)$$ в численном анализе нашли благодаря своему следующему замечательному 
    свойству.

    **Теорема.** Из всех многочленов $$P_n(x)$$ степени $$n$$ со старшим коэффициентом $$a_n = 1$$ у многочлена 
    $$T_n(x)/2^{n-1}$$ точная верхняя грань абсолютных значений на интервале $$[-1, 1]$$ наименьшая, 
    равная $$1/2^{n-1}$$.

    **Доказательство.** Рассмотрим разность:
    """)
    st.latex(r"""
    \varphi(x) = \frac{1}{2^{n-1}}T_n(x) - P_n(x),
    """)
    st.markdown("""
    где $$P_n(x)$$ – произвольный многочлен, удовлетворяющий условиям теоремы.

    Очевидно, что $$\\varphi(x)$$ – многочлен степени не выше $$n-1$$. Так как $$T_n(x)$$ есть 
    $$\\cos(n\\theta),$$ где $$\\theta = \\arccos x$$, то при $$-1 \\leq x \\leq 1$$ $$T_n(x)$$ принимает экстремальные 
    значения $$(\\pm1)(n+1)$$ раз (по очереди положительное и отрицательное). 
    Если у $$P_n(x)$$ верхняя граница экстремальных абсолютных значений меньше, чем у $$T_n/2^{n-1},$$ то 
    $$\\varphi(x)$$ будет иметь в точках экстремумов $$T_n(x)$$ или положительные значения, или отрицательные 
    (поочередно). Следовательно, многочлен $$\\varphi(x)$$ имеет при $$-1 \\leq x \\leq 1$$ $$\\ n$$ нулей, т.е. 
    $$\\varphi(x) \\equiv 0$$ (согласно основной теореме алгебры).

    Таким образом:
    """)
    st.latex(r"""
    P_n(x) = \frac{T_n(x)}{2^{n-1}},
    """)
    st.markdown("что и требовалось.")

    st.markdown("""
    Таким образом, многочлены Чебышёва являются наименее отклоняющимися от нуля на отрезке $$[-1,1]$$ среди всех 
    многочленов в указанном выше смысле. Это свойство многочленов Чебышёва широко используется при рассмотрении 
    различных вопросов вычислительной математики.
    """)

    def chebyshev_polynomial(n, x):
        """Вычисление многочлена Чебышева T_n(x)"""
        return math.cos(n * math.acos(x))

    def plot_chebyshev_polynomials(max_degree):
        """Построение графиков многочленов Чебышева от T_0 до T_max_degree"""
        x_values = [i / 100 for i in range(-100, 101)]  # x от -1 до 1
        plt.figure(figsize=(10, 6))

        for n in range(max_degree + 1):
            y_values = [chebyshev_polynomial(n, x) for x in x_values]
            plt.plot(x_values, y_values, label=f"T_{n}(x)")

        plt.title(f"Многочлены Чебышева до T_{max_degree}(x)")
        plt.xlabel("x")
        plt.ylabel("T_n(x)")
        plt.axhline(0, color="black", linewidth=0.8, linestyle="--")
        plt.axvline(0, color="black", linewidth=0.8, linestyle="--")
        plt.grid(True)
        plt.legend()
        return plt

    # Streamlit UI
    st.markdown("#### Визуальный пример:")

    st.markdown("""
    Многочлены Чебышева $$T_n(x)$$ , как уже было сказано, определяются формулой:
    """)
    st.latex(r"T_n(x) = \cos(n \arccos x)")
    st.markdown("""
    Они ортогональны на отрезке $$[-1, 1]$$.
    """)
    st.markdown("""
    ##### Выберите максимальную степень многочлена (n):
    """)
    # Пользователь выбирает степень многочлена
    max_degree = st.slider("", min_value=0, max_value=10, value=5, step=1)

    # Построение графиков
    st.markdown(f"**Графики многочленов Чебышева от T_0 до T_{max_degree}:**")
    plot = plot_chebyshev_polynomials(max_degree)
    st.pyplot(plot)

    st.markdown("""
    Многочлены Чебышева обладают замечательным свойством наименьшего отклонения от нуля среди всех многочленов 
    одной и той же степени на отрезке $$[-1, 1]$$. Это делает их полезными в задачах приближения и интерполяции.
    """)


# 6.5. Итерационные методы решения линейных систем
def iterative_methods_linear_systems():
    st.header("6.5. Итерационные методы решения линейных систем")

    st.markdown("""
    При решении уравнений в частных производных конечно-разностными методами часто возникают алгебраические системы 
    с тысячами неизвестных, но со слабо заполненной матрицей $$A$$, в которой большинство элементов – нули. 
    Для таких систем прямые методы становятся невыгодными, так как они не позволяют, вообще говоря, воспользоваться. 
    Такие системы удобно решать итерационными методами. Мы на них подробнее остановимся при рассмотрении 
    конечно-разностных методов для уравнений в частных производных, а здесь лишь обсудим два метода, которые скорее 
    важны в теоретическом плане, а в практических вычислениях используются редко.

    Первый – метод простых итераций. При этом методе система приводится к виду:
    """)
    st.latex(r"""
    \overline{x} = C \overline{x} + \overline{d} \tag{24}
    """)
    st.markdown("""
    и итерационный процесс можно записать как
    """)
    st.latex(r"""
    \overline{x}^{(S+1)} = C \overline{x}^{(S)} + \overline{d}. 
    """)
    st.markdown("""
    Очевидно:
    """)
    st.latex(r"""
    \overline{x}^{(S+1)} - \overline{x}^{(S)} = C \left( \overline{x}^{(S)} - \overline{x}^{(S-1)} \right),
    """)
    st.latex(r"""
    \left\| \overline{x}^{(S+1)} - \overline{x}^{(S)} \right\| \leq \|C\| \left\| \overline{x}^{(S)} - 
    \overline{x}^{(S-1)} \right\|.
    """)
    st.markdown("""
    Поэтому итерации будут заведомо сходиться (и независимо от начального приближения), если $$\\|C\\| < 1$$.

    Представление в виде $$(24)$$ можно осуществить по-разному, например, выделив диагональные элементы:
    """)
    st.latex(r"""
    x_l = \frac{1}{a_{ll}} \left( b_l - \sum_{i \neq l} a_{li} x_i \right), \quad l = 1, 2, \dots, n. \tag{25}
    """)
    st.markdown("""
    В этой записи легко учесть специальную структуру матрицы $$A$$, суммируя лишь ненулевые элементы. При использовании 
    различных норм условия сходимости имеют вид:
    """)
    st.latex(r"""
    \max_l \sum_{i \neq l} \left| \frac{a_{li}}{a_{ll}} \right| < 1; \quad \max_i \sum_{l \neq i} 
    \frac{|a_{li}|}{|a_{ll}|} < 1; \quad \sum_{l}\sum_{i \neq l} \frac{|a_{li}|^2}{|a_{ll}|^2} < 1. \tag{26}
    """)
    st.markdown("""
    Эти условия означают диагональное преобладание в матрице. Если метод сходится, то решение для $$(24)$$ существует, и 
    единственное, поэтому диагональное преобладание, гарантирующее сходимость, означает, что $$det A \\neq 0$$.
    """)

    st.markdown("""
    #### Рассмотрим пример реализации этого метода:
    """)

    st.code("""
    def simple_iteration(A, b, x0, tol, max_iter):
        '''Метод простых итераций для решения линейной системы Ax = b'''
        n = len(b)
        x = x0[:]
        norm_diff = []
    
        for _ in range(max_iter):
            x_new = [0] * n
            for i in range(n):
                x_new[i] = sum(A[i][j] * x[j] for j in range(n)) + b[i]
    
            # Вычисление нормы разности между итерациями
            diff = sum((x_new[i] - x[i])**2 for i in range(n))**0.5
            norm_diff.append(diff)
    
            if diff < tol:
                break
    
            x = x_new[:]
    
        return x, norm_diff
    
    # Пример
    A = [[0, 0.2], [0.3, 0]]
    b = [0.8, 1.2]
    x0 = [0, 0]
    tol = 1e-5
    max_iter = 100
    
    x_result, norm_diff = simple_iteration(A, b, x0, tol, max_iter)
    
    # График сходимости
    print("Результат метода простых итераций:", x_result)
    print("Сходимость (простые итерации):")
    for i, diff in enumerate(norm_diff):
        print(f"Итерация {i + 1}: Норма разности = {diff:.6f}")      
    """)

    st.markdown("""
    Иногда в итерационном процессе, основанном на записи $$(25)$$, в правой части для неизвестных берётся уже с новой 
    итерации. В этом случае имеем:
    """)
    st.latex(r"""
    x_l^{(S+1)} = \frac{1}{a_{ll}} \left[ b_l - \sum_{i=1}^{l-1} a_{li} x_i^{(S+1)} - \sum_{i=l+1}^n a_{li} x_i^{(S)} 
    \right].
    """)
    st.markdown("""
    Такой метод называется методом Зейделя.
    """)

    st.markdown("#### Рассмотрим этот пример:")

    st.code("""
    def seidel_method(A, b, x0, tol, max_iter):
        '''Метод Зейделя для решения линейной системы Ax = b'''
        n = len(A)
        x = x0[:]
        norm_diff = []
    
        for _ in range(max_iter):
            x_new = x[:]
            for i in range(n):
                sum1 = sum(A[i][j] * x_new[j] for j in range(i))  # Используем уже обновленные значения
                sum2 = sum(A[i][j] * x[j] for j in range(i + 1, n))
                x_new[i] = (b[i] - sum1 - sum2) / A[i][i]
    
            # Вычисление нормы разности между итерациями
            diff = sum((x_new[i] - x[i])**2 for i in range(n))**0.5
            norm_diff.append(diff)
    
            if diff < tol:
                break
    
            x = x_new[:]
    
        return x, norm_diff
    
    # Пример
    A = [[4, 1], [2, 3]]
    b = [1, 2]
    x0 = [0, 0]
    tol = 1e-5
    max_iter = 100
    
    x_result, norm_diff = seidel_method(A, b, x0, tol, max_iter)
    
    # График сходимости
    print("Результат метода Зейделя:", x_result)
    print("Сходимость (Зейдель):")
    for i, diff in enumerate(norm_diff):
        print(f"Итерация {i + 1}: Норма разности = {diff:.6f}")
    """)

    def simple_iteration(A, b, x0, max_iter):
        """Метод простых итераций для решения линейной системы Ax = b"""
        n = len(b)
        x = x0[:]
        tol = 1e-5  # Фиксированная точность
        norm_diff = []

        for _ in range(max_iter):
            x_new = [0] * n
            for i in range(n):
                x_new[i] = sum(A[i][j] * x[j] for j in range(n)) + b[i]

            # Вычисление нормы разности между итерациями
            diff = sum((x_new[i] - x[i]) ** 2 for i in range(n)) ** 0.5
            norm_diff.append(diff)

            if diff < tol:
                break

            x = x_new[:]

        return x, norm_diff

    def seidel_method(A, b, x0, max_iter):
        """Метод Зейделя для решения линейной системы Ax = b"""
        n = len(A)
        x = x0[:]
        tol = 1e-5  # Фиксированная точность
        norm_diff = []

        for _ in range(max_iter):
            x_new = x[:]
            for i in range(n):
                sum1 = sum(A[i][j] * x_new[j] for j in range(i))  # Используем уже обновленные значения
                sum2 = sum(A[i][j] * x[j] for j in range(i + 1, n))
                x_new[i] = (b[i] - sum1 - sum2) / A[i][i]

            # Вычисление нормы разности между итерациями
            diff = sum((x_new[i] - x[i]) ** 2 for i in range(n)) ** 0.5
            norm_diff.append(diff)

            if diff < tol:
                break

            x = x_new[:]

        return x, norm_diff

    st.markdown("""
    Сравнение двух методов

    Для сравнения вы можете ввести матрицу $$A$$, вектор $$b$$, начальное приближение $$x_0$$, а также задать 
    максимальное число итераций.
    """)

    # Ввод данных
    A_str = st.text_area("Введите матрицу A построчно, элементы через пробел (например, '4 1\\n2 3'):", "4 1\n2 3")
    b_str = st.text_input("Введите вектор b через пробел (например, '1 2'):", "1 2")
    x0_str = st.text_input("Введите начальное приближение x0 через пробел (например, '0 0'):", "0 0")
    max_iter = st.number_input("Максимальное число итераций:", min_value=1, max_value=1000, value=100)

    # Обработка данных
    try:
        A = [[float(num) for num in row.split()] for row in A_str.strip().split("\n")]
        b = [float(num) for num in b_str.split()]
        x0 = [float(num) for num in x0_str.split()]

        if len(A) != len(b) or any(len(row) != len(A) for row in A):
            raise ValueError("Размеры матрицы A и вектора b не соответствуют.")
    except Exception as e:
        st.error(f"Ошибка ввода: {e}")
        st.stop()

    try:
        x_exact = np.linalg.solve(A, b)
    except Exception as e:
        st.error(f"Ошибка при вычислении правильного решения: {e}")
        st.stop()

    # Кнопка вычисления
    if st.button("Вычислить"):
        # Метод простых итераций
        x_simple, norm_diff_simple = simple_iteration(A, b, x0, max_iter)
        st.subheader("Метод простых итераций")
        st.markdown(f"**Решение:** {x_simple}")
        st.markdown(f"**Число итераций:** {len(norm_diff_simple)}")
        st.line_chart(norm_diff_simple, height=300, use_container_width=True)

        # Метод Зейделя
        x_seidel, norm_diff_seidel = seidel_method(A, b, x0, max_iter)
        st.subheader("Метод Зейделя")
        st.markdown(f"**Решение:** {x_seidel}")
        st.markdown(f"**Число итераций:** {len(norm_diff_seidel)}")
        st.line_chart(norm_diff_seidel, height=300, use_container_width=True)

        # Правильное решение
        st.subheader("Правильное решение")
        st.markdown(f"**Решение с использованием numpy.linalg.solve:** {x_exact}")

    code_editor()

    st.markdown("""
    **Замечание 1.** Условия $$(26)$$ достаточно жесткие. Однако для любой системы $$A\\overline{x} = \\overline{b}$$ с 
    $$\\det A \\neq 0$$ можно всегда (теоретически!) сделать преобразование её к виду:
    """)
    st.latex(r"""
    \overline{x} = C\overline{x} + \overline{d}, \tag{27}
    """)
    st.markdown("""
    где $$\\|C\\| < 1$$. В самом деле, составим матрицу $$D = A^{-1} - \\varepsilon,$$ где $$\\varepsilon = 
    (\\varepsilon_{ij})$$ – 
    матрица с малыми элементами, и умножим исходную систему на нее:
    """)
    st.latex(r"""
    (A^{-1} - \varepsilon)A\overline{x} = (A^{-1} - \varepsilon)\overline{b}.
    """)
    st.markdown("""
    Отсюда имеем $$(27)$$ c $$C = \\varepsilon A; \\, \\overline{d} = (A^{-1} - \\varepsilon)\\overline{b}.$$ 
    
    Очевидно, всегда можно выбрать $$\\varepsilon$$ так, чтобы выполнилось одно из условий сходимости итераций:
    """)
    st.latex(r"""
    \max_i \sum_{j=1}^n |C_{ij}| < 1; \quad \max_j \sum_{i=1}^n |C_{ij}| < 1; \quad \left(\sum_{i,j} |C_{ij}|^2 
    \right)^{1/2} < 1.
    """)
    st.markdown("""
    На практике систему $$A\\overline{x} = \\overline{b}$$ преобразуют, комбинируя уравнения так, чтобы выполнялись 
    условия диагонального преобладания $$(26)$$.

    **Замечание 2.** Метод Зейделя обычно даёт более быструю сходимость, чем метод простых итераций. Однако можно 
    привести примеры, когда метод простых итераций сходится, а метод Зейделя – нет, и наоборот, т. е. условия 
    сходимости этих методов, вообще говоря, различны.

    **Пример.** Пусть
    """)
    st.latex(r"""
    C = \begin{pmatrix} -p & q \\ -q & p \end{pmatrix},
    """)
    st.markdown("""
    обозначим разность между двумя итерациями через $$\\overline{\\delta}.$$ Тогда для простых итераций:
    """)
    st.latex(r"""
    \delta_1^{(S)} = p \delta_1^{(S-1)} + q \delta_2^{(S-1)};
    """)
    st.latex(r"""
    \delta_2^{(S)} = -q \delta_1^{(S-1)} + p \delta_2^{(S-1)}. \tag{28}
    """)
    st.markdown("""
    Для итераций по Зейделю:
    """)
    st.latex(r"""
    \delta_1^{(S)} = p \delta_1^{(S-1)} + q \delta_2^{(S-1)};
    """)
    st.latex(r"""
    \delta_2^{(S)} = -q \delta_1^{(S)} + p \delta_2^{(S-1)}. \tag{29}
    """)

    st.markdown("""
    Можно показать (см. [1], с. 400), что при $$p = -0,5, \\quad q = 0,6$$ $$(28)$$ сходится, $$(29)$$ – нет; при 
    $$p = 0,5, \\quad q = 1 \\ (29)$$ сходится, $$(28)\\ $$ – нет. Доказательство этих фактов основано на изучении 
    собственных чисел соответствующих матриц.

    Для $$(28)$$ имеем:
    """)
    st.latex(r"""
    \left| \begin{array}{cc} p - \lambda & q \\ -q & p - \lambda \end{array} \right| = 0 
    \implies \lambda^2 - 2p\lambda + p^2 + q^2 = 0 \implies \lambda = p \pm qi \implies |\lambda| < 1
    """)
    st.markdown("""
    при
    """)
    st.latex(r"""
    p^2 + q^2 < 1.
    """)
    st.markdown("""
    Первая пара $$(p, q)$$ этому условию удовлетворяет, вторая – нет.

    Для $$(29)$$:
    """)
    st.latex(r"""
    \left| \begin{array}{cc} p - \lambda & -q \\ -pq & p - \lambda - q^2 \end{array} \right| = 0 \implies 
    \lambda^2 - 2p\lambda + p^2 + q^2 = 0 \implies \lambda = p - \frac{q^2}{2} \pm \sqrt{\frac{1}{4} - p} \cdot q.
    """)
    st.markdown("""
    Для первой пары $$(p, q)$$:
    """)
    st.latex(r"""
    \lambda = -0,68 \pm 0,6 \sqrt{0,75},
    """)
    st.markdown("""
    условие $$|\\lambda| < 1$$ не выполняется. Для второй пары:
    """)
    st.latex(r"""
    \lambda = \pm 0,5i; \quad |\lambda| = 0,5 < 1.
    """)
